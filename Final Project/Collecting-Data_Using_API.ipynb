{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDA0321ENSkillsNetwork21426264-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Collecting Job Data Using APIs**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **45 to 60** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this lab, you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Collect job data from Jobs API\n",
    "*   Store the collected data into an excel spreadsheet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <strong>Note: Before starting with the assignment make sure to read all the instructions and then move ahead with the coding part.</strong>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the actual lab, firstly you need to click on the [Jobs_API](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/labs/module%201/Accessing%20Data%20Using%20APIs/Jobs_API.ipynb) notebook link. The file contains flask code which is required to run the Jobs API data.\n",
    "\n",
    "Now, to run the code in the file that opens up follow the below steps.\n",
    "\n",
    "Step1: Download the file.\n",
    "\n",
    "Step2: Upload it on the IBM Watson studio. (If IBM Watson Cloud service does not work in your system, follow the alternate Step 2 below)\n",
    "\n",
    "Step2(alternate): Upload it in your SN labs environment using the upload button which is highlighted in red in the image below:\n",
    "Remember to upload this Jobs_API file in the same folder as your current .ipynb file\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/labs/module%201/Accessing%20Data%20Using%20APIs/Upload.PNG\" />\n",
    "\n",
    "Step3:  Run all the cells of the Jobs_API file. (Even if you receive an asterik sign after running the last cell, the code works fine.)\n",
    "\n",
    "If you want to learn more about flask, which is optional, you can click on this link [here](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/labs/module%201/Accessing%20Data%20Using%20APIs/FLASK_API.md.html).\n",
    "\n",
    "Once you run the flask code, you can start with your assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Used in this Assignment\n",
    "\n",
    "The dataset used in this lab comes from the following source: [https://www.kaggle.com/promptcloud/jobs-on-naukricom](https://www.kaggle.com/promptcloud/jobs-on-naukricom?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDA0321ENSkillsNetwork21426264-2022-01-01) under the under a **Public Domain license**.\n",
    "\n",
    "> Note: We are using a modified subset of that dataset for the lab, so to follow the lab instructions successfully please use the dataset provided with the lab, rather than the dataset from the original source.\n",
    "\n",
    "The original dataset is a csv. We have converted the csv to json as per the requirement of the lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-Up Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you attempt the actual lab, here is a fully solved warmup exercise that will help you to learn how to access an API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an API, let us find out who currently are on the International Space Station (ISS).<br> The API at [http://api.open-notify.org/astros.json](http://api.open-notify.org/astros.json?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDA0321ENSkillsNetwork21426264-2021-01-01&cm_mmc=Email_Newsletter-\\_-Developer_Ed%2BTech-\\_-WW_WW-\\_-SkillsNetwork-Courses-IBM-DA0321EN-SkillsNetwork-21426264&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ) gives us the information of astronauts currently on ISS in json format.<br>\n",
    "You can read more about this API at [http://open-notify.org/Open-Notify-API/People-In-Space/](http://open-notify.org/Open-Notify-API/People-In-Space?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDA0321ENSkillsNetwork21426264-2021-01-01&cm_mmc=Email_Newsletter-\\_-Developer_Ed%2BTech-\\_-WW_WW-\\_-SkillsNetwork-Courses-IBM-DA0321EN-SkillsNetwork-21426264&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # you need this module to make an API call\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = \"http://api.open-notify.org/astros.json\" # this url gives use the astronaut data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(api_url) # Call the API using the get method and store the\n",
    "                                # output of the API call in a variable called response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if response.ok:             # if all is well() no errors, no network timeouts)\n",
    "    data = response.json()  # store the result in json format in a variable called data\n",
    "                            # the variable data is of type dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number': 10, 'people': [{'name': 'Oleg Artemyev', 'craft': 'ISS'}, {'name': 'Denis Matveev', 'craft': 'ISS'}, {'name': 'Sergey Korsakov', 'craft': 'ISS'}, {'name': 'Kjell Lindgren', 'craft': 'ISS'}, {'name': 'Bob Hines', 'craft': 'ISS'}, {'name': 'Samantha Cristoforetti', 'craft': 'ISS'}, {'name': 'Jessica Watkins', 'craft': 'ISS'}, {'name': 'Cai Xuzhe', 'craft': 'Tiangong'}, {'name': 'Chen Dong', 'craft': 'Tiangong'}, {'name': 'Liu Yang', 'craft': 'Tiangong'}], 'message': 'success'}\n"
     ]
    }
   ],
   "source": [
    "print(data)   # print the data just to check the output or for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of astronauts currently on ISS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(data.get('number'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the names of the astronauts currently on ISS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 astronauts on ISS\n",
      "And their names are :\n",
      "Oleg Artemyev\n",
      "Denis Matveev\n",
      "Sergey Korsakov\n",
      "Kjell Lindgren\n",
      "Bob Hines\n",
      "Samantha Cristoforetti\n",
      "Jessica Watkins\n",
      "Cai Xuzhe\n",
      "Chen Dong\n",
      "Liu Yang\n"
     ]
    }
   ],
   "source": [
    "astronauts = data.get('people')\n",
    "print(\"There are {} astronauts on ISS\".format(len(astronauts)))\n",
    "print(\"And their names are :\")\n",
    "for astronaut in astronauts:\n",
    "    print(astronaut.get('name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope the warmup was helpful. Good luck with your next lab!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://remotive.com/api/remote-jobs?category=data&search=junior&limit=5'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accessing API with jobs on Remotive.com  --> the link  https://remotive.com/api/remote-jobs\n",
    "# the parameters are category, company_name, search, limit\n",
    "\n",
    "params1 = {'category':\"data\", \"search\":'junior', 'limit': 5}\n",
    "url1 = 'https://remotive.com/api/remote-jobs'\n",
    "\n",
    "r=requests.get(url1,params=params1, verify=True)\n",
    "r.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'00-warning': 'Remotive main domain moved to remotive.com ! Please make your API calls on remotive.com/api/remote-jobs instead of remotive.io now ;) Legacy endpoint remotive.io/api/remote-jobs will be terminated in June 2022. Thank you!',\n",
       " '0-legal-notice': \"Legal warning - Hey, thanks for using Remotive's API, we appreciate it! Please note that API documentation and access is granted so that developers can share our jobs further. Please do not submit Remotive jobs to third Party websites, including but not limited to: Jooble, Neuvoo, Google Jobs, LinkedIn Jobs. Please link back to the URL found on Remotive AND mention Remotive as a source in order to Remotive to get traffic from your listing. If you don't do that, we'll terminate your API access, sorry! Jobs displayed are delayed by 24 hours, the goal being that jobs are attributed to Remotive on various platforms. Displaying our jobs in order to collect signups/email addresses to show a listing constitutes a breach of our terms of services. We offer a private, paid-for API, please email us at hello(at)remotive(dot)io for more information (starting budget is $5k/mo). Please find out terms of services on https://remotive.com/api-documentation. Please note that there is absolutely no need to request Remotive Job data too frequently. Typically, you only need to GET Remotive job data through this API a couple of times a day (we advise max. 4 times a day). Our data is not changing much faster than that anyway. Note that excessive requests will be blocked. Many thanks (Rodolphe & the Remotive team!)\",\n",
       " 'job-count': 2,\n",
       " 'jobs': [{'id': 1236484,\n",
       "   'url': 'https://remotive.com/remote-jobs/data/data-engineer-1236484',\n",
       "   'title': 'Data Engineer',\n",
       "   'company_name': 'Petlab Co.',\n",
       "   'company_logo': 'https://remotive.com/job/1236484/logo',\n",
       "   'category': 'Data',\n",
       "   'tags': ['amazon',\n",
       "    'api',\n",
       "    'cloud',\n",
       "    'docker',\n",
       "    'python',\n",
       "    'sales',\n",
       "    'sql',\n",
       "    'growth',\n",
       "    'management',\n",
       "    'design',\n",
       "    'Engineering',\n",
       "    'community',\n",
       "    'mentoring',\n",
       "    'data',\n",
       "    'business',\n",
       "    'terraform',\n",
       "    'front',\n",
       "    'GCP',\n",
       "    'warehouse',\n",
       "    'people',\n",
       "    'development',\n",
       "    'reporting',\n",
       "    'IT',\n",
       "    'implementation',\n",
       "    'applications',\n",
       "    'github',\n",
       "    'support',\n",
       "    'diversity',\n",
       "    'software'],\n",
       "   'job_type': 'full_time',\n",
       "   'publication_date': '2022-05-29T15:40:38',\n",
       "   'candidate_required_location': 'UK',\n",
       "   'salary': '',\n",
       "   'description': '<p><strong>Title: Data Engineer</strong></p>\\n<p><strong>Location: UK Remote</strong></p>\\n<p><strong>Position: Perm</strong></p>\\n<p>\\xa0</p>\\n<p><strong>About Petlab Co</strong></p>\\n<p>\\xa0</p>\\n<p>Petlab Co. is the world’s fastest-growing pet supplement business. Launching in 2018, Petlab Co. is currently on track to drive over $120m in sales in 2022 without any external funding. They have a loyal community of over 120,000+ pets and over 250 thousand owners have had their lives positively impacted by innovative Petlab Co. products. Petlab Co. is a vibrant, fast-moving, and customer-centric team who value and reward high performing players. It is an ideal place for a candidate who is ambitious, hardworking, loves animals, is looking to rapidly progress their career, and would enjoy working in a vibrant environment with smart co-workers and friendly office dogs!</p>\\n<p>\\xa0</p>\\n<p><strong>A Day In The Life…</strong></p>\\n<p>\\xa0</p>\\n<p>Reporting to the Head of Data you’ll join a small but growing team of highly talented engineers in which you will be key in growing Petlab Co.’s data engineering capabilities. You’ll have plenty of autonomy and flexibility in your role with plenty of opportunity for growth and development. You’ll also take on the responsibility of mentoring one of our junior high-flying engineers.</p>\\n<p>We move and think fast so it’s important that you are able to thrive in a fast-paced environment and are happy to roll up your sleeves to do whatever it takes to support the team in every way possible. This role is fully remote within the UK however we do have a really cool office space in central London if you do want to visit. If that sounds exciting - we would love you to join our pack!</p>\\n<p>Your day to day will look a little bit like this…</p>\\n<ul>\\n<li>Improve and optimise our custom API connections (*Most are managed by our modelling software, Y42, but we do have some containerised custom connections (Amazon, ShipBob, Aftership).</li>\\n<li>Model our source data into a data warehouse that supports the use cases of the wider business (Star Schema/Dimensional modelling).</li>\\n<li>Support our new Snowplow implementation, ensuring we maximise on its benefits for the business.</li>\\n<li>Design and implement data quality monitoring systems.</li>\\n<li>Collaborate with the internal stakeholders to understand how engineering can better support colleagues.</li>\\n<li>Assist in the management and optimisation of our cloud environment (GCP).</li>\\n<li>Help lead a transition to an IAAC (Terraform) cloud environment.</li>\\n</ul>\\n<p><strong>Requirements</strong></p>\\n<p><strong>What You Need…</strong></p>\\n<p>\\xa0</p>\\n<ul>\\n<li>At least 2 years working as a Data Engineer</li>\\n<li>Proven history of pipeline management</li>\\n<li>Proven abilities to code (Send us your Github if you have one!)</li>\\n<li>Desire to lead from the front and think on their own two feet</li>\\n<li>Understanding of dimensional modelling</li>\\n<li>A grasp of IAAC concepts</li>\\n</ul>\\n<p><strong>Technical Skills</strong></p>\\n<ul>\\n<li>Excellent SQL and Python</li>\\n<li>Experience with Cloud (Ideally GCP)</li>\\n</ul>\\n<p><strong>Desirable</strong></p>\\n<ul>\\n<li>Containerisation/Docker</li>\\n<li>Data vis</li>\\n<li>Snowplow</li>\\n</ul>\\n<p>\\xa0</p>\\n<p><strong>Benefits</strong></p>\\n<p><strong>The Good Stuff…</strong></p>\\n<ul>\\n<li>Generous Annual Leave - 28 days + public and bank holidays</li>\\n<li>Flexible Working Hours – We focus on results and trust people to manage their time, whether working from home, while traveling, or in the office!</li>\\n<li>Company Equipment – Laptop issued to you so you\\'re all set from day 1.</li>\\n<li>Get Your Read On – We love to promote and encourage personal &amp; professional development so we couldn\\'t be happier to buy books of your choice via amazon or pay for audible subscription .</li>\\n<li>Employee Assistance Programme.</li>\\n<li>Pension Scheme – contributions towards your pension scheme!</li>\\n<li>Enhanced Maternity / Paternity / Adoption Leave – because time with new family members is important!</li>\\n<li>Puppy Therapy – working in partnership with Paws in Work to provide a boost of oxytocin every quarter.</li>\\n<li>Cycle2Work Scheme – Government initiative which offers a cost-effective way to buy new cycling equipment.</li>\\n<li>Discount Vouchers &amp; Gym Memberships – get discounts at a whole range of retailers and gym memberships through our provider, Sodexo.</li>\\n<li>Free breakfast, fruits and snacks – refuel and revitalise with free munchies in the office.</li>\\n<li>Environment – dogs are welcome!</li>\\n</ul>\\n<p>\\xa0</p>\\n<p>Petlab Co is an equal opportunity employer that is committed to diversity and inclusion. We encourage all applications irrespective of gender, race, sexual orientation, religion, age, nationality, marital status, and disability. We believe that diversity is at the heart of innovation and we welcome passionate candidates form all backgrounds. Come join the family and see for yourself!</p>\\n<img src=\"https://remotive.com/job/track/1236484/blank.gif?source=public_api\" alt=\"\"/>'},\n",
       "  {'id': 1160603,\n",
       "   'url': 'https://remotive.com/remote-jobs/data/data-scientist-1160603',\n",
       "   'title': 'Data Scientist',\n",
       "   'company_name': 'Cytora',\n",
       "   'company_logo': 'https://remotive.com/job/1160603/logo',\n",
       "   'category': 'Data',\n",
       "   'tags': ['analyst',\n",
       "    'AWS',\n",
       "    'data science',\n",
       "    'git',\n",
       "    'machine learning',\n",
       "    'python',\n",
       "    'growth',\n",
       "    'Engineering',\n",
       "    'product',\n",
       "    'data analysis',\n",
       "    'knowledge',\n",
       "    'mentoring',\n",
       "    'deployment',\n",
       "    'data',\n",
       "    'UNIX',\n",
       "    'front',\n",
       "    'ETL',\n",
       "    'development',\n",
       "    'startup',\n",
       "    'IT',\n",
       "    'learning',\n",
       "    'applications',\n",
       "    'problem-solving',\n",
       "    'data-driven',\n",
       "    'search',\n",
       "    'communication',\n",
       "    'production',\n",
       "    'risk',\n",
       "    'insurance',\n",
       "    'software'],\n",
       "   'job_type': 'full_time',\n",
       "   'publication_date': '2022-03-29T14:20:55',\n",
       "   'candidate_required_location': 'UK',\n",
       "   'salary': '',\n",
       "   'description': '<p><strong>Description</strong></p>\\n<p>We are a high-growth startup using data and machine learning to revolutionise the insurance industry. You will be joining an established team, working to build products that are fundamentally changing the way insurers see the world, enabling them to move from an assumption based understanding of risk, to an empirical, data-driven view.</p>\\n<p>\\xa0</p>\\n<p><strong>What working as a Data Scientist at Cytora looks like</strong></p>\\n<p>Cytora’s Resolve Team is an interdisciplinary team, and you will be working alongside data engineers, software engineers and data scientists. The team is responsible for entity resolution as well as for ingesting, maintaining and making usable sector specific datasets.</p>\\n<p>We have a challenging pipeline that requires you to work outside the box and beyond the state of the art. We therefore need you to have a deep understanding of how the technology you’re using works and its limits. We are also a team of builders, so you will be coming up with solutions rather than algorithms - owning your idea, communicating with product, engineering and your coworkers to develop and deploy it.</p>\\n<p><strong>Requirements</strong></p>\\n<ul>\\n<li>2-4 years of proven experience as a Data Scientist/Data Analyst or Data Engineer</li>\\n<li>Ideally experience working with or developing search engines</li>\\n<li>Relevance tuning search results</li>\\n<li>Experience with working with a variety of data, ideally external data</li>\\n<li>Ability to perform exploratory data analysis and data processing</li>\\n<li>Ability to draw insights from noisy data when facing open ended problems</li>\\n<li>Ingestion pipelines and ETL</li>\\n<li>Experience with production environments where data science applications were deployed and served to customers live, ideally AWS</li>\\n<li>Experience with Python, version control (Git) and unix based systems; preferably have some hands-on engineering/deployment experience</li>\\n<li>Ability to work in a fast moving environment</li>\\n<li>Drive to share knowledge with peers and empower your team</li>\\n<li>Problem-solving and inquisitive mind and a self-starter attitude</li>\\n</ul>\\n<p>Bonus points:</p>\\n<ul>\\n<li>Experience working in a startup environment</li>\\n<li>Experience with entity resolution, entity linking and knowledge of how tools such as the DBpedia Spotlight service work</li>\\n<li>Excited about engineering tasks</li>\\n<li>Experience mentoring junior data scientists</li>\\n<li>Communication skills to present technical results in front of internal and external stakeholders</li>\\n</ul>\\n<p><strong>Benefits</strong></p>\\n<ul>\\n<li>Remote first</li>\\n<li>Competitive salary</li>\\n<li>Share options</li>\\n<li>£2000 work abroad budget each year</li>\\n<li>£1500 Learning and development budget each year</li>\\n<li>Flexible hours</li>\\n<li>Private Health Insurance</li>\\n<li>Enhanced parental leave</li>\\n</ul>\\n<img src=\"https://remotive.com/job/track/1160603/blank.gif?source=public_api\" alt=\"\"/>'}]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = r.json()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - This is a position of Data Engineer in Petlab Co. with decription: \n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Title: Data Engineer</strong></p>\n",
       "<p><strong>Location: UK Remote</strong></p>\n",
       "<p><strong>Position: Perm</strong></p>\n",
       "<p> </p>\n",
       "<p><strong>About Petlab Co</strong></p>\n",
       "<p> </p>\n",
       "<p>Petlab Co. is the world’s fastest-growing pet supplement business. Launching in 2018, Petlab Co. is currently on track to drive over $120m in sales in 2022 without any external funding. They have a loyal community of over 120,000+ pets and over 250 thousand owners have had their lives positively impacted by innovative Petlab Co. products. Petlab Co. is a vibrant, fast-moving, and customer-centric team who value and reward high performing players. It is an ideal place for a candidate who is ambitious, hardworking, loves animals, is looking to rapidly progress their career, and would enjoy working in a vibrant environment with smart co-workers and friendly office dogs!</p>\n",
       "<p> </p>\n",
       "<p><strong>A Day In The Life…</strong></p>\n",
       "<p> </p>\n",
       "<p>Reporting to the Head of Data you’ll join a small but growing team of highly talented engineers in which you will be key in growing Petlab Co.’s data engineering capabilities. You’ll have plenty of autonomy and flexibility in your role with plenty of opportunity for growth and development. You’ll also take on the responsibility of mentoring one of our junior high-flying engineers.</p>\n",
       "<p>We move and think fast so it’s important that you are able to thrive in a fast-paced environment and are happy to roll up your sleeves to do whatever it takes to support the team in every way possible. This role is fully remote within the UK however we do have a really cool office space in central London if you do want to visit. If that sounds exciting - we would love you to join our pack!</p>\n",
       "<p>Your day to day will look a little bit like this…</p>\n",
       "<ul>\n",
       "<li>Improve and optimise our custom API connections (*Most are managed by our modelling software, Y42, but we do have some containerised custom connections (Amazon, ShipBob, Aftership).</li>\n",
       "<li>Model our source data into a data warehouse that supports the use cases of the wider business (Star Schema/Dimensional modelling).</li>\n",
       "<li>Support our new Snowplow implementation, ensuring we maximise on its benefits for the business.</li>\n",
       "<li>Design and implement data quality monitoring systems.</li>\n",
       "<li>Collaborate with the internal stakeholders to understand how engineering can better support colleagues.</li>\n",
       "<li>Assist in the management and optimisation of our cloud environment (GCP).</li>\n",
       "<li>Help lead a transition to an IAAC (Terraform) cloud environment.</li>\n",
       "</ul>\n",
       "<p><strong>Requirements</strong></p>\n",
       "<p><strong>What You Need…</strong></p>\n",
       "<p> </p>\n",
       "<ul>\n",
       "<li>At least 2 years working as a Data Engineer</li>\n",
       "<li>Proven history of pipeline management</li>\n",
       "<li>Proven abilities to code (Send us your Github if you have one!)</li>\n",
       "<li>Desire to lead from the front and think on their own two feet</li>\n",
       "<li>Understanding of dimensional modelling</li>\n",
       "<li>A grasp of IAAC concepts</li>\n",
       "</ul>\n",
       "<p><strong>Technical Skills</strong></p>\n",
       "<ul>\n",
       "<li>Excellent SQL and Python</li>\n",
       "<li>Experience with Cloud (Ideally GCP)</li>\n",
       "</ul>\n",
       "<p><strong>Desirable</strong></p>\n",
       "<ul>\n",
       "<li>Containerisation/Docker</li>\n",
       "<li>Data vis</li>\n",
       "<li>Snowplow</li>\n",
       "</ul>\n",
       "<p> </p>\n",
       "<p><strong>Benefits</strong></p>\n",
       "<p><strong>The Good Stuff…</strong></p>\n",
       "<ul>\n",
       "<li>Generous Annual Leave - 28 days + public and bank holidays</li>\n",
       "<li>Flexible Working Hours – We focus on results and trust people to manage their time, whether working from home, while traveling, or in the office!</li>\n",
       "<li>Company Equipment – Laptop issued to you so you're all set from day 1.</li>\n",
       "<li>Get Your Read On – We love to promote and encourage personal &amp; professional development so we couldn't be happier to buy books of your choice via amazon or pay for audible subscription .</li>\n",
       "<li>Employee Assistance Programme.</li>\n",
       "<li>Pension Scheme – contributions towards your pension scheme!</li>\n",
       "<li>Enhanced Maternity / Paternity / Adoption Leave – because time with new family members is important!</li>\n",
       "<li>Puppy Therapy – working in partnership with Paws in Work to provide a boost of oxytocin every quarter.</li>\n",
       "<li>Cycle2Work Scheme – Government initiative which offers a cost-effective way to buy new cycling equipment.</li>\n",
       "<li>Discount Vouchers &amp; Gym Memberships – get discounts at a whole range of retailers and gym memberships through our provider, Sodexo.</li>\n",
       "<li>Free breakfast, fruits and snacks – refuel and revitalise with free munchies in the office.</li>\n",
       "<li>Environment – dogs are welcome!</li>\n",
       "</ul>\n",
       "<p> </p>\n",
       "<p>Petlab Co is an equal opportunity employer that is committed to diversity and inclusion. We encourage all applications irrespective of gender, race, sexual orientation, religion, age, nationality, marital status, and disability. We believe that diversity is at the heart of innovation and we welcome passionate candidates form all backgrounds. Come join the family and see for yourself!</p>\n",
       "<img src=\"https://remotive.com/job/track/1236484/blank.gif?source=public_api\" alt=\"\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 - This is a position of Data Scientist in Cytora with decription: \n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Description</strong></p>\n",
       "<p>We are a high-growth startup using data and machine learning to revolutionise the insurance industry. You will be joining an established team, working to build products that are fundamentally changing the way insurers see the world, enabling them to move from an assumption based understanding of risk, to an empirical, data-driven view.</p>\n",
       "<p> </p>\n",
       "<p><strong>What working as a Data Scientist at Cytora looks like</strong></p>\n",
       "<p>Cytora’s Resolve Team is an interdisciplinary team, and you will be working alongside data engineers, software engineers and data scientists. The team is responsible for entity resolution as well as for ingesting, maintaining and making usable sector specific datasets.</p>\n",
       "<p>We have a challenging pipeline that requires you to work outside the box and beyond the state of the art. We therefore need you to have a deep understanding of how the technology you’re using works and its limits. We are also a team of builders, so you will be coming up with solutions rather than algorithms - owning your idea, communicating with product, engineering and your coworkers to develop and deploy it.</p>\n",
       "<p><strong>Requirements</strong></p>\n",
       "<ul>\n",
       "<li>2-4 years of proven experience as a Data Scientist/Data Analyst or Data Engineer</li>\n",
       "<li>Ideally experience working with or developing search engines</li>\n",
       "<li>Relevance tuning search results</li>\n",
       "<li>Experience with working with a variety of data, ideally external data</li>\n",
       "<li>Ability to perform exploratory data analysis and data processing</li>\n",
       "<li>Ability to draw insights from noisy data when facing open ended problems</li>\n",
       "<li>Ingestion pipelines and ETL</li>\n",
       "<li>Experience with production environments where data science applications were deployed and served to customers live, ideally AWS</li>\n",
       "<li>Experience with Python, version control (Git) and unix based systems; preferably have some hands-on engineering/deployment experience</li>\n",
       "<li>Ability to work in a fast moving environment</li>\n",
       "<li>Drive to share knowledge with peers and empower your team</li>\n",
       "<li>Problem-solving and inquisitive mind and a self-starter attitude</li>\n",
       "</ul>\n",
       "<p>Bonus points:</p>\n",
       "<ul>\n",
       "<li>Experience working in a startup environment</li>\n",
       "<li>Experience with entity resolution, entity linking and knowledge of how tools such as the DBpedia Spotlight service work</li>\n",
       "<li>Excited about engineering tasks</li>\n",
       "<li>Experience mentoring junior data scientists</li>\n",
       "<li>Communication skills to present technical results in front of internal and external stakeholders</li>\n",
       "</ul>\n",
       "<p><strong>Benefits</strong></p>\n",
       "<ul>\n",
       "<li>Remote first</li>\n",
       "<li>Competitive salary</li>\n",
       "<li>Share options</li>\n",
       "<li>£2000 work abroad budget each year</li>\n",
       "<li>£1500 Learning and development budget each year</li>\n",
       "<li>Flexible hours</li>\n",
       "<li>Private Health Insurance</li>\n",
       "<li>Enhanced parental leave</li>\n",
       "</ul>\n",
       "<img src=\"https://remotive.com/job/track/1160603/blank.gif?source=public_api\" alt=\"\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#download IPYTHON display HTML to show desription correctly \n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "jobs = data.get('jobs')\n",
    "for numb, job in enumerate(jobs):\n",
    "    descr = job.get('description')\n",
    "    print(f\"{numb+1} - This is a position of {job.get('title')} in {job.get('company_name')} with decription: \\n \")\n",
    "    display(HTML(descr))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Collect Jobs Data using Jobs API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective: Determine the number of jobs currently open for various technologies  and for various locations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the number of job postings for the following locations using the API:\n",
    "\n",
    "*   Los Angeles\n",
    "*   New York\n",
    "*   San Francisco\n",
    "*   Washington DC\n",
    "*   Seattle\n",
    "*   Austin\n",
    "*   Detroit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a function to get the number of jobs for the Python technology.<br>\n",
    "\n",
    "> Note: While using the lab you need to pass the **payload** information for the **params** attribute in the form of **key** **value** pairs.\n",
    "\n",
    "Refer the ungraded **rest api lab** in the course **Python for Data Science, AI & Development**  <a href=\"https://www.coursera.org/learn/python-for-applied-data-science-ai/ungradedLti/P6sW8/hands-on-lab-access-rest-apis-request-http?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDA0321ENSkillsNetwork21426264-2022-01-01\">link</a>\n",
    "\n",
    "##### The keys in the json are\n",
    "\n",
    "*   Job Title\n",
    "\n",
    "*   Job Experience Required\n",
    "\n",
    "*   Key Skills\n",
    "\n",
    "*   Role Category\n",
    "\n",
    "*   Location\n",
    "\n",
    "*   Functional Area\n",
    "\n",
    "*   Industry\n",
    "\n",
    "*   Role\n",
    "\n",
    "You can also view  the json file contents  from the following <a href = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/labs/module%201/Accessing%20Data%20Using%20APIs/jobs.json?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDA0321ENSkillsNetwork21426264-2022-01-01\">json</a> URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url=\"http://127.0.0.1:5000/data\"\n",
    "def get_number_of_jobs_T(technology):\n",
    "    payload ={'Key Skills':technology}\n",
    "    data = requests.get(api_url,params=payload)\n",
    "    jobs=data.json()\n",
    "    number_of_jobs = len(jobs)\n",
    "    print(number_of_jobs)\n",
    "    return technology,number_of_jobs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the function for Python and checking if it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Python', 1173)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_number_of_jobs_T(\"Python\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a function to find number of jobs in US for a location of your choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_jobs_L(location):\n",
    "    payload ={'Location':location}\n",
    "    data = requests.get(api_url,params=payload)\n",
    "    jobs=data.json()\n",
    "    number_of_jobs = len(jobs)\n",
    "    print(number_of_jobs)\n",
    "   \n",
    "    return location,number_of_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the function for Los Angeles and check if it is working.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Boston', 2966)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code goes here\n",
    "get_number_of_jobs_L(\"Boston\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the results in an excel file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the API for all the given technologies above and write the results in an excel spreadsheet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not know how create excel file using python, double click here for **hints**.\n",
    "\n",
    "<!--\n",
    "\n",
    "from openpyxl import Workbook        # import Workbook class from module openpyxl\n",
    "wb=Workbook()                        # create a workbook object\n",
    "ws=wb.active                         # use the active worksheet\n",
    "ws.append(['Country','Continent'])   # add a row with two columns 'Country' and 'Continent'\n",
    "ws.append(['Eygpt','Africa'])        # add a row with two columns 'Egypt' and 'Africa'\n",
    "ws.append(['India','Asia'])          # add another row\n",
    "ws.append(['France','Europe'])       # add another row\n",
    "wb.save(\"countries.xlsx\")            # save the workbook into a file called countries.xlsx\n",
    "\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a python list of all locations for which you need to find the number of jobs postings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['Los Angeles','New York', 'San Francisco','Washington DC', 'Seattle','Austin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries required to create excel spreadsheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.0.10-py2.py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.0.10\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl\n",
    "from openpyxl import Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a workbook and select the active worksheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb=Workbook() \n",
    "ws=wb.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the number of jobs postings for each of the location in the above list.\n",
    "Write the Location name and the number of jobs postings into the excel spreadsheet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640\n",
      "('Los Angeles', 640)\n",
      "3226\n",
      "('New York', 3226)\n",
      "435\n",
      "('San Francisco', 435)\n",
      "5316\n",
      "('Washington DC', 5316)\n",
      "3375\n",
      "('Seattle', 3375)\n",
      "434\n",
      "('Austin', 434)\n"
     ]
    }
   ],
   "source": [
    "for c in countries:\n",
    "    jobs = get_number_of_jobs_L(c)\n",
    "    #jobs_str = str(jobs)\n",
    "    print(jobs)\n",
    "    ws.append([jobs[0], jobs[1]])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save into an excel spreadsheet named 'job-postings.xlsx'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb.save(\"job-postings.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the similar way, you can try for below given technologies and results  can be stored in an excel sheet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the number of job postings for the following languages using the API:\n",
    "\n",
    "*   C\n",
    "*   C#\n",
    "*   C++\n",
    "*   Java\n",
    "*   JavaScript\n",
    "*   Python\n",
    "*   Scala\n",
    "*   Oracle\n",
    "*   SQL Server\n",
    "*   MySQL Server\n",
    "*   PostgreSQL\n",
    "*   MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13498\n",
      "('C', 13498)\n",
      "333\n",
      "('C#', 333)\n",
      "305\n",
      "('C++', 305)\n",
      "2609\n",
      "('Java', 2609)\n",
      "355\n",
      "('JavaScript', 355)\n",
      "1173\n",
      "('Python', 1173)\n",
      "33\n",
      "('Scala', 33)\n",
      "784\n",
      "('Oracle', 784)\n",
      "250\n",
      "('SQL Server', 250)\n",
      "0\n",
      "('MySQL Server', 0)\n",
      "10\n",
      "('PostgreSQL', 10)\n",
      "174\n",
      "('MongoDB', 174)\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "Technology = ['C','C#', 'C++', 'Java', 'JavaScript' ,'Python','Scala', 'Oracle', 'SQL Server',\n",
    "             'MySQL Server', 'PostgreSQL','MongoDB']\n",
    "wb=Workbook() \n",
    "ws=wb.active\n",
    "for t in Technology:\n",
    "    jobs = get_number_of_jobs_T(t)\n",
    "    #jobs_str = str(jobs)\n",
    "    print(jobs)\n",
    "    ws.append([jobs[0], jobs[1]])\n",
    "    \n",
    "wb.save(\"job-postings-tech.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ayushi Jain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rav Ahuja\n",
    "\n",
    "Lakshmi Holla\n",
    "\n",
    "Malika\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date (YYYY-MM-DD) | Version | Changed By        | Change Description                 |\n",
    "| ----------------- | ------- | ----------------- | ---------------------------------- |\n",
    "| 2022-01-19        | 0.3     | Lakshmi Holla     | Added changes in the markdown      |\n",
    "| 2021-06-25        | 0.2     | Malika            | Updated GitHub job json link       |\n",
    "| 2020-10-17        | 0.1     | Ramesh Sannareddy | Created initial version of the lab |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2022 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
